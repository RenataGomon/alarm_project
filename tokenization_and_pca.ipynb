{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ukraine Conflict Text Analysis with Google Drive Integration\n",
    "\n",
    "This notebook processes the Ukraine conflict data, creates TF-IDF matrices and PCA results, and uploads them to Google Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.11/site-packages (2.1.4)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (1.26.4)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.11/site-packages (1.2.2)\n",
      "Collecting google-auth\n",
      "  Using cached google_auth-2.38.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting google-auth-oauthlib\n",
      "  Downloading google_auth_oauthlib-1.2.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-auth-httplib2\n",
      "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-api-python-client\n",
      "  Using cached google_api_python_client-2.166.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/lib/python3.11/site-packages (from google-auth) (0.2.8)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting httplib2>=0.19.0 (from google-auth-httplib2)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 (from google-api-python-client)\n",
      "  Using cached google_api_core-2.24.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client)\n",
      "  Using cached uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting googleapis-common-protos<2.0.0,>=1.56.2 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client)\n",
      "  Using cached googleapis_common_protos-1.69.2-py3-none-any.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /opt/anaconda3/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.20.3)\n",
      "Collecting proto-plus<2.0.0,>=1.22.3 (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client)\n",
      "  Using cached proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /opt/anaconda3/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.31.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/anaconda3/lib/python3.11/site-packages (from httplib2>=0.19.0->google-auth-httplib2) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth) (0.4.8)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2024.2.2)\n",
      "Using cached google_auth-2.38.0-py2.py3-none-any.whl (210 kB)\n",
      "Downloading google_auth_oauthlib-1.2.1-py2.py3-none-any.whl (24 kB)\n",
      "Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Using cached google_api_python_client-2.166.0-py2.py3-none-any.whl (13.2 MB)\n",
      "Using cached google_api_core-2.24.2-py3-none-any.whl (160 kB)\n",
      "Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Using cached googleapis_common_protos-1.69.2-py3-none-any.whl (293 kB)\n",
      "Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Installing collected packages: uritemplate, rsa, proto-plus, oauthlib, httplib2, googleapis-common-protos, requests-oauthlib, google-auth, google-auth-oauthlib, google-auth-httplib2, google-api-core, google-api-python-client\n",
      "Successfully installed google-api-core-2.24.2 google-api-python-client-2.166.0 google-auth-2.38.0 google-auth-httplib2-0.2.0 google-auth-oauthlib-1.2.1 googleapis-common-protos-1.69.2 httplib2-0.22.0 oauthlib-3.2.2 proto-plus-1.26.1 requests-oauthlib-2.0.0 rsa-4.9 uritemplate-4.1.1\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install pandas numpy nltk scikit-learn google-auth google-auth-oauthlib google-auth-httplib2 google-api-python-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/admin/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/admin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/admin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/admin/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/admin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.util import bigrams\n",
    "import re\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import io\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Service account file not found at /home/ubuntu/jn/air-alarms-data-506614e0f8b8.json\n",
      "You'll need to update the file path or use Option 1 authentication instead.\n"
     ]
    }
   ],
   "source": [
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaInMemoryUpload\n",
    "\n",
    "ROOT_FOLDER_ID = \"1kN_F4168tJQGDnHcC6G7-Lp_Y1Lq2t5A\"  # Replace with your folder ID\n",
    "SERVICE_ACCOUNT_FILE = '/home/ubuntu/jn/air-alarms-data-506614e0f8b8.json'  # Replace with your file path\n",
    "SCOPES = ['https://www.googleapis.com/auth/drive']\n",
    "\n",
    "drive_service = None\n",
    "if os.path.exists(SERVICE_ACCOUNT_FILE):\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        SERVICE_ACCOUNT_FILE, scopes=SCOPES\n",
    "    )\n",
    "    drive_service = build('drive', 'v3', credentials=credentials)\n",
    "    print(\"Google Drive service initialized successfully.\")\n",
    "else:\n",
    "    print(f\"Warning: Service account file not found at {SERVICE_ACCOUNT_FILE}\")\n",
    "    print(\"You'll need to update the file path or use Option 1 authentication instead.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_create_or_get_folder(folder_name, parent_id=None, retries=3):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            return create_or_get_folder(folder_name, parent_id)\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt+1} failed: {e}\")\n",
    "            time.sleep(2)\n",
    "    raise RuntimeError(f\"Failed to create folder '{folder_name}' after {retries} attempts.\")\n",
    "\n",
    "def create_or_get_folder(folder_name, parent_id=None):\n",
    "    query = f\"name='{folder_name}' and mimeType='application/vnd.google-apps.folder'\"\n",
    "    if parent_id:\n",
    "        query += f\" and '{parent_id}' in parents\"\n",
    "    results = drive_service.files().list(q=query, fields=\"files(id, name)\").execute()\n",
    "    folders = results.get('files', [])\n",
    "    if folders:\n",
    "        return folders[0]['id']\n",
    "    file_metadata = {\n",
    "        'name': folder_name,\n",
    "        'mimeType': 'application/vnd.google-apps.folder',\n",
    "        'parents': [parent_id] if parent_id else []\n",
    "    }\n",
    "    folder = drive_service.files().create(body=file_metadata, fields='id').execute()\n",
    "    return folder['id']\n",
    "\n",
    "def upload_csv_to_drive(folder_id, file_name, df):\n",
    "    content = df.to_csv(index=False)\n",
    "    media = MediaInMemoryUpload(content.encode(), mimetype='text/csv')\n",
    "    query = f\"name='{file_name}' and '{folder_id}' in parents\"\n",
    "    existing_files = drive_service.files().list(q=query, fields=\"files(id)\").execute().get('files', [])\n",
    "    if existing_files:\n",
    "        file_id = existing_files[0]['id']\n",
    "        drive_service.files().update(fileId=file_id, media_body=media).execute()\n",
    "        print(f\"Updated existing file: {file_name}\")\n",
    "    else:\n",
    "        file_metadata = {\n",
    "            'name': file_name,\n",
    "            'parents': [folder_id],\n",
    "            'mimeType': 'text/csv'\n",
    "        }\n",
    "        drive_service.files().create(body=file_metadata, media_body=media).execute()\n",
    "        print(f\"Created new file: {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date                                            content\n",
      "0  05-04-2025  Click here to read the full report with maps O...\n",
      "1  04-04-2025  Click here to read the full report with maps K...\n",
      "2  03-04-2025  Click here to read the full report. Nicole Wol...\n",
      "3  02-04-2025  Click here to read the full report Angelica Ev...\n",
      "4  01-04-2025  Click here to read the full report. Angelica E...\n",
      "5  31-03-2025  Click here to read the full report. Nicole Wol...\n",
      "6  30-03-2025  Click here to read the full report with maps O...\n",
      "7  29-03-2025  Click here to read the full report with maps N...\n",
      "8  28-03-2025  Click here to read the full report with maps A...\n",
      "9  27-03-2025  Click here to read the full report. Nicole Wol...\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "custom_stop_phrases = [\n",
    "    \"angelica evan\", \"christina harward\", \"click read\", \"click see\", \"et click\",\n",
    "    \"frederick kagan\", \"full report\", \"karolina hird\", \"isw assess\", \"isw continu\",\n",
    "    \"isw cover\", \"isw interact\", \"isw observ\", \"isw previous\", \"kateryna stepanenko\",\n",
    "    \"key takeaway\", \"map present\", \"map russian\", \"map updat\", \"nicol wolkov\", \"pm et\",\n",
    "    \"present report\", \"previous assess\", \"previous report\", \"read full\", \"riley bailey\",\n",
    "    \"see isw\", \"static map\", \"takeaway russian\", \"timelaps map\"\n",
    "]\n",
    "\n",
    "stemmed_custom_phrases = [\n",
    "    ' '.join([stemmer.stem(word) for word in phrase.split()])\n",
    "    for phrase in custom_stop_phrases\n",
    "]\n",
    "\n",
    "df = pd.read_csv(\"ukraine_conflict_updates.csv\")\n",
    "df = df.dropna(subset=['content'])\n",
    "print(df.head(10))\n",
    "\n",
    "\n",
    "def remove_stemmed_phrases(tokens, phrase_list):\n",
    "    text = ' '.join(tokens)\n",
    "    for phrase in phrase_list:\n",
    "        text = re.sub(rf\"\\b{re.escape(phrase)}\\b\", \"\", text)\n",
    "    return text.split()\n",
    "\n",
    "def clean_and_show_tokens(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "clean_and_show_tokens(df['content'].iloc[0])\n",
    "\n",
    "def clean_and_stem(text):\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', text)\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    stemmed = [stemmer.stem(token) for token in tokens]\n",
    "    cleaned_tokens = remove_stemmed_phrases(stemmed, stemmed_custom_phrases)\n",
    "    \n",
    "    return cleaned_tokens\n",
    "\n",
    "def bigram_tokenizer(text):\n",
    "    tokens = clean_and_stem(text)\n",
    "    return [' '.join(bg) for bg in bigrams(tokens)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date                                            content\n",
      "0  05-04-2025  Click here to read the full report with maps O...\n",
      "1  04-04-2025  Click here to read the full report with maps K...\n",
      "2  03-04-2025  Click here to read the full report. Nicole Wol...\n",
      "3  02-04-2025  Click here to read the full report Angelica Ev...\n",
      "4  01-04-2025  Click here to read the full report. Angelica E...\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create TF-IDF Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created TF–IDF matrix with 1125 rows and 201 columns\n",
      "(1125, 200)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(tokenizer=bigram_tokenizer, max_features=200)\n",
    "X = vectorizer.fit_transform(df['content'])\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_matrix = pd.DataFrame(X.toarray(), columns=feature_names)\n",
    "tfidf_matrix.insert(0, 'date', df['date'].values)\n",
    "tfidf_matrix.to_csv(\"bigram_tfidf_matrix.csv\", index=False)\n",
    "\n",
    "print(f\"Created TF–IDF matrix with {tfidf_matrix.shape[0]} rows and {tfidf_matrix.shape[1]} columns\")\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created PCA result with 1125 rows and 3 columns\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X.toarray())\n",
    "\n",
    "pca_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n",
    "pca_df.insert(0, 'date', df['date'].values)  \n",
    "pca_df.to_csv(\"pca_tfidf_result.csv\", index=False)\n",
    "\n",
    "print(f\"Created PCA result with {pca_df.shape[0]} rows and {pca_df.shape[1]} columns\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Files to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_files_to_drive_service_account():\n",
    "    if drive_service is None:\n",
    "        print(\"Google Drive service not initialized. Check your service account credentials.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        results_folder_id = safe_create_or_get_folder(\"isw_vectors\", parent_id=ROOT_FOLDER_ID)\n",
    "\n",
    "        upload_csv_to_drive(results_folder_id, \"bigram_tfidf_matrix.csv\", tfidf_matrix)\n",
    "        upload_csv_to_drive(results_folder_id, \"pca_tfidf_result.csv\", pca_df)\n",
    "\n",
    "        print(f\"All files uploaded successfully to folder: isw_vectors\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading files: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_files_to_drive_colab():\n",
    "    try:\n",
    "        \n",
    "        folder_path = f\"/content/drive/MyDrive/isw_vectors\"\n",
    "        \n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        \n",
    "        tfidf_matrix.to_csv(f\"{folder_path}/bigram_tfidf_matrix.csv\", index=False)\n",
    "        pca_df.to_csv(f\"{folder_path}/pca_tfidf_result.csv\", index=False)\n",
    "        \n",
    "        print(f\"All files uploaded successfully to Google Drive at: {folder_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error uploading files: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Upload to Google Drive\n",
    "\n",
    "Choose and run one of the following cells depending on your authentication method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error uploading files: name 'tfidf_matrix' is not defined\n"
     ]
    }
   ],
   "source": [
    "upload_files_to_drive_service_account()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
